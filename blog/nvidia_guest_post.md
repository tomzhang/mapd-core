[TODO: replace this with actual intro]

At MapD our goal is to build the world's fastest big data analytics and visualization platform that allows for lag-free interactive exploration of multi-billion row datasets.  Although we are fast running on X86, our real advantage stems from our ability to leverage the massive parallelism and memory bandwidth of GPUs. 

Even the fastest hardware will be slow if it is not targeted properly, so at MapD we have invested heavily in ensuring a wide range of analytic workloads run optimally on GPUs. In particular, we have worked hard so that common SQL analytic operations, such as filter and group by, run as fast as possible.  One of the biggest payoffs in this regard has been moving from the query interpreter used in our prototype to a JIT compilation framework built on LLVM. 

On the CPU,  there are quite a few systems, some of them [open source](https://github.com/cloudera/impala), using either [LLVM](http://llvm.org) for generating native code or the source-to-source translation approach (typically SQL to C++). We decided to go the LLVM route, which has the following advantages over an an interpreter:

 - With an interpreter, we'd have to use intermediate buffers for expression evaluation since the execution dispatch loop must work on many elements at the time (running it for every row would be prohibitively expensive). This would have to happen in shared memory or L1 cache, which we'd rather use for storing the results of a query's `GROUP BY` clause.
 - An efficient ad-hoc interpreter would probably need a few subtle tricks and maybe a byte-code format. We can side-step these concerns, at least for now, by doing a single pass translation from the query syntax tree to [LLVM Intermediate Representation (IR)](http://llvm.org/docs/LangRef.html), leaving most of the low-level optimizations to LLVM.
 - The generated code encapsulates a lot of information on how the query executes. We can compare the LLVM IR we generate before and after a change to the code-base, gaining additional confidence in the correctness of it. We've taken advantage of this for a few refactorings and it'd have been significantly harder with an interpreter.
 - Depending on the number and range of the columns used in a `GROUP BY` clause, different hash strategies are optimal. Some of them rely on generating collision-free hash functions based on the range of the data, which is only known at runtime.
 - LLVM comes with built-in code validation APIs and tools. For example, comparison and arithmetic operations on integers will fail (with an useful error message) if their widths are different. Once a function is complete, `llvm::verifyFunction` will do additional sanity checks -- making sure, among other things, that the control flow graph of our query is well-formed.

Depending on the nature of the implemented language, compilation time constraints, security concerns and many other factors, native code generation can be significantly harder than an interpreter. Fortunately, for the subset of SQL we support (and plan to) we didn't find this to be the case.

### How we're using NVVM

LLVM might be great and battle-proven for CPUs, but our product is focused on GPUs. If we could use it for GPU code compilation; we'd get all the benefits we've already mentioned while being able to compare the two hardware platforms (CPU and GPU) in a meaningful way.

Fortunately, NVIDIA had made this a reality long before we started to build our product. [NVVM IR](http://docs.nvidia.com/cuda/nvvm-ir-spec) is a rather extensive subset of LLVM IR. Today, our system runs on GPU, my GPU-less ultrabook and the 32-bit ARM on the Jetson TK1 using the same code-base. We've even found and fixed bugs on the CPU while they were still latent on the GPU.

Just a word of caution - despite sharing the IR specification, NVVM and LLVM are ultimately different code-bases. Going with an older version of LLVM, preferably the one NVVM is based on, can help. We decided against that approach since the LLVM API offers a wide range of "IR surgery" features and we were able to fix up these mismatches, but your mileage may vary.

Also, unlike LLVM IR, unaligned loads are not allowed in NVVM IR. The address of a load must be a multiple of the size of the type; otherwise, the query would crash with an invalid memory access error on the GPU, even if the load is not annotated as aligned.

Not all the code needs to be directly generated by us. We offload some of the functionality to a runtime written in C++ whenever code generation would be tedious and error-prone without any performance benefits. This approach is a great fit for aggregate functions, arithmetic on nullable values, hash dictionaries and more. The LLVM based C++ compiler, [clang](http://clang.llvm.org/), generates the corresponding LLVM IR we then combine with the code we generate explicitly. Most of the runtime functions are marked as always inline, which means they won't be present as individual functions in the final IR, after the `AlwaysInliner` optimization pass in LLVM runs. Inlining also means no function call overhead and increased scope for other optimization passes. For example, a reasonable way of implementing a `max` aggregate is shown below.

```
extern "C" __attribute__((always_inline))
void agg_max(int64_t* agg, const int64_t val) {
  *agg = std::max(*agg, val);
}
```

Note that the function is not marked as `__device__` since this is not CUDA C++ code. Any explicit call to this function will be eventually inlined and the result can run unmodified on the GPU. Also, if `agg` points to a value allocated on the stack (as is the case for queries without `GROUP BY` clause), the `PromoteMemoryToRegister` pass will place it in a register for the inner loop of the query. The runtime functions which need to be different for the GPU are part of a regular CUDA C++ library we can call from the query.

As it's always the case when compilation is involved, the time required to generate native code is an important question. An interactive system will likely see new queries all the time as the user refines them in search of insight. We're able to keep it consistently under 30 ms for entirely new queries, which we think is good enough to be unnoticeable in the console, especially for massive datasets. However, for "mere billions" of rows, our UI is able to show smooth animations over multiple correlated charts. Since the actual execution is so fast in this case, 30 ms can matter a lot. Fortunately, these queries are structurally identical and only differ in the value of literals as the time window moves across the time range or the user selects the tail of a histogram. With caching in place, the compilation time becomes a non-issue. We keep it simple and still generate the IR, then use it as a key in the native code cache. The LLVM API offers an easy way to serialize source level entities (functions in our case), shown below.

```
std::string serialize_function(const llvm::Function* f) {
  std::stringstream ss;
  llvm::raw_os_ostream os(ss);
  f->print(os);
  return ss.str();
}
```

We've said that NVVM generates native code, but there actually is an additional step we haven't discussed. From the IR we generate, NVVM generates [PTX](http://docs.nvidia.com/cuda/parallel-thread-execution/), which in turn is compiled to native code for the GPU. Especially if you're bundling a CUDA C++ library with the generated code, like we do, [caching](http://devblogs.nvidia.com/parallelforall/cuda-pro-tip-understand-fat-binaries-jit-caching/) the result of this last step is very important. Make sure the compute cache directory is writable by your application or else it will silently fail and recompile every time. The code snippet below shows how we bundle a library with the PTX we generate.

```
checkCudaErrors(cuLinkCreate(num_options, &option_keys[0],
  &option_values[0], &link_state_));
if (!lib_path.empty()) {
  // To create a static CUDA library:
  // 1. nvcc -std=c++11 -arch=sm_30 --device-link
  //      -c [list of .cu files]
  // 2. nvcc -std=c++11 -arch=sm_30
  //     -lib [list of .o files generated by step 1]
  //     -o [library_name.a]
  checkCudaErrors(cuLinkAddFile(link_state_, CU_JIT_INPUT_LIBRARY,
    lib_path.c_str(), num_options, &option_keys[0], &option_values[0]));
}
checkCudaErrors(cuLinkAddData(link_state_, CU_JIT_INPUT_PTX,
  static_cast<void*>(ptx), strlen(ptx) + 1,
  0, num_options, &option_keys[0], &option_values[0]));
void* cubin;
size_t cubin_size;
checkCudaErrors(cuLinkComplete(link_state_, &cubin, &cubin_size));
checkCudaErrors(cuModuleLoadDataEx(&module_, cubin, num_options,
  &option_keys[0], &option_values[0]));
checkCudaErrors(cuModuleGetFunction(&kernel_, module_, func_name.c_str()));
```

There is an upper bound for the number of registers a block can use, so the `CU_JIT_THREADS_PER_BLOCK` option should be set to the block size. Failing to do so can make the translation to native code fail. We've had this issue for queries with many projected columns and a lot of threads per block before setting this option.

Speaking of libraries, general purpose computing on GPUs doesn't mean the C POSIX functions will be available. In our case, we needed `gmtime_r` for the `EXTRACT` family of SQL functions. Fortunately, we've been able to port it from [newlib](https://sourceware.org/newlib/) and compile it with NVCC.

### Performance measurements and comments 

As it's always the case with performance-focused systems, the ideas might be great but the proof is in the pudding. It turns out we're able to get a lot of performance from the GPU.

Queries using filter and aggregate routinely hit more than 80% of the available bandwidth. We've measured more than 240 GB/s on a single K40 for a filter and count query touching a single column. Grouping by a single column with 20 possible values and some skew (the carrier in the airline data set) can only hit slightly more than 100 GB/s. On the new Titan X GPU, based on the Maxwell architecture, we were able to get more than 200 GB/s on the same query, on a single card. It looks like shared atomics in Maxwell handle contention significantly better than the Kepler generation, which explains this great result on skewed inputs. We're looking forward to this feature being implemented on future generations of Tesla cards as well.

While we're able to get a 40-50x speedup, on a multi-GPU system, even when compared to our own code running on a high end dual-socket CPU system, there are queries for which the gap is two orders of magnitude (often code with lots of divisions, which tend to be slow on X64). And compared to other leading in-memory CPU-based databases, which typically use interpreters or transpilers, the speedup can easily be three orders of magnitude. 

Creating a SQL JIT for GPUs is just one of the many optimizations we've implemented to make MapD as fast as possible. If you'd like to learn more, please visit our [website](http://mapd.com) and download our whitepaper.
